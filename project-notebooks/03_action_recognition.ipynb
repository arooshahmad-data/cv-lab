{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIB0tlSGO8YZ",
        "outputId": "fda9d926-90fa-466c-92dd-997d9fc553d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6Xb2Kzd5nw2"
      },
      "outputs": [],
      "source": [
        "# !pip install einops --quiet\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#------- torch imports -----------#\n",
        "import torch\n",
        "from torch import nn\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "#------------- Video Testing Imports -----------#\n",
        "import time\n",
        "from PIL import Image\n",
        "from IPython.display import display, clear_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEzf0fiy2Ez6"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb5rpeBk5rux"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    DOWNLOAD_DATASET = False  # Change to False after first download\n",
        "    DATASET_NAME = \"matthewjansen/ucf101-action-recognition\"\n",
        "    LOCAL_FOLDER_NAME = \"ucf101_data\"\n",
        "\n",
        "    SELECTED_CLASSES = 10 # select only first 10 classes.\n",
        "    SEQUENCE_LENGTH = 30\n",
        "    IMAGE_SIZE = 32\n",
        "    CHANNELS = 3\n",
        "    BATCH_SIZE = 4\n",
        "\n",
        "    MODEL = \"SingleFrameCNN\"\n",
        "    NUM_EPOCHS = 100\n",
        "    SAVE_EVERY = 1\n",
        "    LEARNING_RATE = 1e-4\n",
        "\n",
        "    SAVE_PATH = f\"/path_to_save_model\"\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    @property\n",
        "    def DATASET_PATH(self):\n",
        "        return os.path.join(self.SAVE_PATH, self.LOCAL_FOLDER_NAME)\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUqqQHM32G1l"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPnWlLFU_v8a"
      },
      "outputs": [],
      "source": [
        "# Set KaggleHub path early\n",
        "try:\n",
        "    import kagglehub\n",
        "except ImportError:\n",
        "    !pip install kagglehub\n",
        "    import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXjD-MT08MEF"
      },
      "outputs": [],
      "source": [
        "if config.DOWNLOAD_DATASET:\n",
        "    print(\"Downloading dataset from KaggleHub...\")\n",
        "    dataset_path = kagglehub.dataset_download(config.DATASET_NAME)\n",
        "    print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "    dest_path = config.DATASET_PATH\n",
        "    shutil.copytree(dataset_path, dest_path, dirs_exist_ok=True)\n",
        "    print(f\"Dataset copied to: {dest_path}\")\n",
        "else:\n",
        "    print(f\"Using existing dataset at: {config.DATASET_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxsamyll2Jvi"
      },
      "source": [
        "## Process Video / Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLFXf36hGDD_"
      },
      "outputs": [],
      "source": [
        "# Custom PyTorch Dataset class for video data\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        # Initialize dataset with video data and corresponding labels\n",
        "        self.data = data        # data is a NumPy array of shape (N, T, H, W, C)\n",
        "        self.labels = labels    # labels are one-hot encoded or categorical labels\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples in the dataset\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fetch one sample (video + label) at the given index\n",
        "\n",
        "        # Convert the video frames from NumPy array to a PyTorch tensor\n",
        "        # and permute dimensions to match PyTorch's expected input format:\n",
        "        # from (T, H, W, C) to (T, C, H, W)\n",
        "        video = torch.tensor(self.data[idx]).permute(0, 3, 1, 2)\n",
        "\n",
        "        # Convert the corresponding label to a tensor\n",
        "        label = torch.tensor(self.labels[idx])\n",
        "\n",
        "        return video, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOdnUnWN2WmY"
      },
      "outputs": [],
      "source": [
        "def process_video(file_path):\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(file_path)\n",
        "    frames = []\n",
        "\n",
        "    # Read frames until we reach the desired sequence length\n",
        "    while len(frames) < config.SEQUENCE_LENGTH:\n",
        "        ret, frame = cap.read()  # Read a frame\n",
        "        if not ret:  # If no frame is returned, end of video reached or error\n",
        "            break\n",
        "        # Resize frame to fixed size and normalize pixel values to [0,1]\n",
        "        frame = cv2.resize(frame, (config.IMAGE_SIZE, config.IMAGE_SIZE)) / 255.0\n",
        "        frames.append(frame)  # Append processed frame to list\n",
        "\n",
        "    cap.release()  # Release the video capture resource\n",
        "\n",
        "    # Return frames as a NumPy array only if we have the exact required number of frames\n",
        "    return np.array(frames) if len(frames) == config.SEQUENCE_LENGTH else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtAqt2bN5vZI"
      },
      "outputs": [],
      "source": [
        "def load_data(split_folder=\"train\", num_classes=None):\n",
        "    # Construct the path to the split folder inside the dataset directory\n",
        "    base_path = os.path.join(config.DATASET_PATH, split_folder)\n",
        "\n",
        "    # List all class directories inside this split folder and sort them alphabetically\n",
        "    action_classes = sorted([d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))])\n",
        "\n",
        "    # Use the provided number of classes or fallback to config.SELECTED_CLASSES\n",
        "    selected_classes = num_classes if num_classes is not None else config.SELECTED_CLASSES\n",
        "    selected_action_classes = action_classes[:selected_classes]\n",
        "\n",
        "    print(f\"\\n[{split_folder.upper()}] Found {len(selected_action_classes)} Classes:\")\n",
        "    print(\"\\n\".join(f\" - {cls}\" for cls in selected_action_classes))\n",
        "\n",
        "    X, y = [], []  # Initialize lists for videos (X) and labels (y)\n",
        "\n",
        "    # Loop through each selected action class folder\n",
        "    for i, action in enumerate(selected_action_classes):\n",
        "        action_path = os.path.join(base_path, action)\n",
        "        if not os.path.isdir(action_path):\n",
        "            continue\n",
        "\n",
        "        for file in os.listdir(action_path):\n",
        "            video_path = os.path.join(action_path, file)\n",
        "\n",
        "            if not video_path.lower().endswith(('.avi', '.mp4', '.mov')):\n",
        "                continue\n",
        "\n",
        "            video_data = process_video(video_path)\n",
        "\n",
        "            if video_data is not None:\n",
        "                X.append(video_data)\n",
        "                y.append(i)\n",
        "\n",
        "    X = np.array(X, dtype=np.float32)\n",
        "    y = to_categorical(y, num_classes=len(selected_action_classes))\n",
        "\n",
        "    return X, y, len(selected_action_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwL5KZLq0H3Z"
      },
      "source": [
        "#### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QwCCvtE9EOv",
        "outputId": "48081ae2-b870-4343-f02b-7d516e759477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test  test.csv\ttrain  train.csv  val  val.csv\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/MyDrive/ucf101_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IYPO65I96mIC",
        "outputId": "a3463268-a241-4c32-d992-29ea52c2adff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TRAIN] Found 10 Classes:\n",
            " - ApplyEyeMakeup\n",
            " - ApplyLipstick\n",
            " - Archery\n",
            " - BabyCrawling\n",
            " - BalanceBeam\n",
            " - BandMarching\n",
            " - BaseballPitch\n",
            " - Basketball\n",
            " - BasketballDunk\n",
            " - BenchPress\n",
            "\n",
            "[VAL] Found 10 Classes:\n",
            " - ApplyEyeMakeup\n",
            " - ApplyLipstick\n",
            " - Archery\n",
            " - BabyCrawling\n",
            " - BalanceBeam\n",
            " - BandMarching\n",
            " - BaseballPitch\n",
            " - Basketball\n",
            " - BasketballDunk\n",
            " - BenchPress\n",
            "\n",
            "[TEST] Found 10 Classes:\n",
            " - ApplyEyeMakeup\n",
            " - ApplyLipstick\n",
            " - Archery\n",
            " - BabyCrawling\n",
            " - BalanceBeam\n",
            " - BandMarching\n",
            " - BaseballPitch\n",
            " - Basketball\n",
            " - BasketballDunk\n",
            " - BenchPress\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "X_train, y_train, num_classes = load_data(\"train\", num_classes = 10)\n",
        "X_val, y_val, _ = load_data(\"val\")\n",
        "X_test, y_test, _ = load_data(\"test\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(VideoDataset(X_train, y_train), batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(VideoDataset(X_val, y_val), batch_size=config.BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(VideoDataset(X_test, y_test), batch_size=config.BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHKtrPGVE5Ew",
        "outputId": "ab025e43-b184-499e-8ed9-1e94363a5fe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train samples: 1125\n",
            "Val samples: 186\n",
            "Test samples: 194\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nTrain samples: {len(X_train)}\")\n",
        "print(f\"Val samples: {len(X_val)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXZAcw4Ty25b"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9brxhk2w2xd-"
      },
      "source": [
        "### SimpleFrameCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4Hr_jK75xEf"
      },
      "outputs": [],
      "source": [
        "# --- SingleFrameCNN ---\n",
        "class SingleFrameCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Convolutional layers to extract features from a single frame (3 channels RGB)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),  # Conv layer with 32 filters, kernel size 3x3, padding=1 keeps spatial size\n",
        "            nn.ReLU(),                       # Activation\n",
        "            nn.MaxPool2d(2),                 # Downsample by factor of 2 (height & width)\n",
        "            nn.Conv2d(32, 64, 3, padding=1),# Second conv layer increasing depth to 64\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        # Fully connected layers for classification\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 8 * 8, 128),     # Flattened feature size after conv layers (assuming input 32x32)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)     # Output layer for classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0]  # Take only the first frame of the video sequence: shape (batch, C, H, W)\n",
        "        x = self.conv(x)  # Extract spatial features from that frame\n",
        "        x = x.reshape(x.size(0), -1)  # Flatten for fully connected layers\n",
        "        return self.fc(x)  # Return class logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhYu4TF220Yi"
      },
      "source": [
        "### C3DLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXAAgYVm2g-x"
      },
      "outputs": [],
      "source": [
        "# --- C3DLite ---\n",
        "class C3DLite(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # 3D conv layers extract spatiotemporal features from video clips\n",
        "        self.conv3d = nn.Sequential(\n",
        "            nn.Conv3d(3, 64, kernel_size=3, padding=1),  # 3D Conv with 64 filters, kernel 3x3x3\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(2),                             # Downsample temporal + spatial dims by 2\n",
        "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(2),\n",
        "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(2),\n",
        "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(2),\n",
        "        )\n",
        "        self.fc = None  # Will initialize after knowing input size dynamically\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Permute from (B, T, C, H, W) to (B, C, T, H, W) for Conv3d\n",
        "        x = x.permute(0, 2, 1, 3, 4)\n",
        "        x = self.conv3d(x)  # Apply 3D conv layers\n",
        "        x = x.reshape(x.size(0), -1)  # Flatten\n",
        "\n",
        "        if self.fc is None:\n",
        "            # Dynamically create fully connected layers based on flattened size\n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(x.shape[1], 512),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(512, num_classes)\n",
        "            )\n",
        "            self.fc.to(x.device)  # Move to same device as input tensor\n",
        "        return self.fc(x)  # Output classification logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_tf7jYU234A"
      },
      "source": [
        "### CNN LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDkddmxp2nwm"
      },
      "outputs": [],
      "source": [
        "# --- CNNLSTM ---\n",
        "class CNNLSTM(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # CNN to extract spatial features frame-wise\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        # Calculate CNN output feature size dynamically for input of size 32x32\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, 3, 32, 32)\n",
        "            cnn_out = self.cnn(dummy_input)\n",
        "            cnn_out_size = cnn_out.numel()  # Total features after CNN\n",
        "\n",
        "        # LSTM to model temporal dependencies across frames\n",
        "        self.lstm = nn.LSTM(input_size=cnn_out_size, hidden_size=128, batch_first=True, num_layers=2)\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C, H, W = x.size()\n",
        "        # Combine batch and time dims for CNN processing of individual frames\n",
        "        x = x.view(B * T, C, H, W)\n",
        "        x = self.cnn(x)  # Extract spatial features\n",
        "        x = x.reshape(B, T, -1)  # Reshape back to (batch, time, features) for LSTM\n",
        "\n",
        "        x, _ = self.lstm(x)  # Apply LSTM over time dimension\n",
        "        return self.fc(x[:, -1])  # Use last LSTM output for classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ZVnmLv25yq"
      },
      "source": [
        "### Slow Fast Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wD913f72uc5"
      },
      "outputs": [],
      "source": [
        "# --- SlowFast ---\n",
        "class SlowFast(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define conv blocks for both Fast and Slow pathways\n",
        "        def conv_stack():\n",
        "          return nn.Sequential(\n",
        "              nn.Conv3d(3, 64, 3, padding=1),\n",
        "              nn.ReLU(),\n",
        "              nn.MaxPool3d((1, 2, 2)),\n",
        "              nn.Conv3d(64, 128, 3, padding=1),\n",
        "              nn.ReLU(),\n",
        "              nn.MaxPool3d((1, 2, 2)),\n",
        "              nn.Conv3d(128, 256, 3, padding=1),\n",
        "              nn.ReLU(),\n",
        "              nn.MaxPool3d((1, 2, 2)),\n",
        "              nn.Conv3d(256, 256, 3, padding=1),\n",
        "              nn.ReLU(),\n",
        "              nn.AdaptiveAvgPool3d((1, 1, 1))  # Fixed output size: (C, 1, 1, 1)\n",
        "          )\n",
        "\n",
        "        self.fast_conv = conv_stack()  # Fast pathway conv layers\n",
        "        self.slow_conv = conv_stack()  # Slow pathway conv layers\n",
        "\n",
        "        # Dynamically compute output feature size for concatenation\n",
        "        with torch.no_grad():\n",
        "            dummy_fast = torch.zeros(1, 3, 16, 32, 32)  # Fast input has more frames\n",
        "            dummy_slow = torch.zeros(1, 3, 8, 32, 32)   # Slow input has fewer frames\n",
        "            f = self.fast_conv(dummy_fast)\n",
        "            s = self.slow_conv(dummy_slow)\n",
        "            fast_dim = f.numel()\n",
        "            slow_dim = s.numel()\n",
        "\n",
        "        # Fully connected layers for combined features\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(fast_dim + slow_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, time, channels, height, width)\n",
        "\n",
        "        # Sample frames for fast pathway (every 2nd frame)\n",
        "        fast = x[:, ::2].permute(0, 2, 1, 3, 4)  # Reorder to (B, C, T, H, W)\n",
        "        # Sample frames for slow pathway (every 4th frame)\n",
        "        slow = x[:, ::4].permute(0, 2, 1, 3, 4)\n",
        "\n",
        "        f = self.fast_conv(fast)  # Fast path features\n",
        "        s = self.slow_conv(slow)  # Slow path features\n",
        "\n",
        "        f = f.reshape(f.size(0), -1)  # Flatten\n",
        "        s = s.reshape(s.size(0), -1)  # Flatten\n",
        "\n",
        "        x = torch.cat((f, s), dim=1)  # Concatenate features from both paths\n",
        "        return self.fc(x)  # Final classification output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgUTk-zSLjpP"
      },
      "source": [
        "## Training Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ntnlK27Ghpe"
      },
      "outputs": [],
      "source": [
        "def get_model_instance(model_name, num_classes):\n",
        "    if model_name == \"SingleFrameCNN\":\n",
        "        return SingleFrameCNN(num_classes=num_classes)\n",
        "    elif model_name == \"C3DLite\":\n",
        "        return C3DLite(num_classes=num_classes)\n",
        "    elif model_name == \"CNNLSTM\":\n",
        "        return CNNLSTM(num_classes=num_classes)\n",
        "    elif model_name == \"SlowFast\":\n",
        "        return SlowFast(num_classes=num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Model '{model_name}' is not supported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBzb0avy3rM9"
      },
      "source": [
        "#### Train and Validate Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSNTfnqm6hXp"
      },
      "outputs": [],
      "source": [
        "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, model_name=None, patience=5):\n",
        "    # Move model to the specified device (GPU or CPU)\n",
        "    model.to(config.DEVICE)\n",
        "\n",
        "    # Create directory to save models and logs\n",
        "    save_path = os.path.join(config.SAVE_PATH, model_name)\n",
        "    os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "    # File to store training logs for later analysis\n",
        "    log_file = os.path.join(save_path, \"training_log.txt\")\n",
        "\n",
        "    # Lists to track loss and accuracy for each epoch\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "\n",
        "    best_val_loss = float('inf')  # Initialize best validation loss to a large number\n",
        "    patience_counter = 0          # For early stopping\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        model.train()  # Set model to training mode\n",
        "        total_loss = 0\n",
        "        correct_train, total_train = 0, 0\n",
        "\n",
        "        # Iterate over batches in the training data\n",
        "        for videos, labels in train_loader:\n",
        "            # Move data to the device\n",
        "            videos, labels = videos.to(config.DEVICE), labels.to(config.DEVICE).float()\n",
        "\n",
        "            optimizer.zero_grad()          # Reset gradients\n",
        "            outputs = model(videos)        # Forward pass\n",
        "            loss = criterion(outputs, labels)  # Compute loss\n",
        "            loss.backward()                # Backpropagation\n",
        "            optimizer.step()               # Update weights\n",
        "\n",
        "            total_loss += loss.item()      # Accumulate loss for averaging\n",
        "\n",
        "            # Calculate number of correct predictions\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            targets = torch.argmax(labels, dim=1)\n",
        "            correct_train += (preds == targets).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        # Average loss and accuracy for training\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "        # Validation phase - no gradient calculation\n",
        "        model.eval()\n",
        "        val_loss, correct_val, total_val = 0, 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for videos, labels in val_loader:\n",
        "                videos, labels = videos.to(config.DEVICE), labels.to(config.DEVICE).float()\n",
        "                outputs = model(videos)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                targets = torch.argmax(labels, dim=1)\n",
        "                correct_val += (preds == targets).sum().item()\n",
        "                total_val += labels.size(0)\n",
        "\n",
        "        # Average loss and accuracy for validation\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = 100 * correct_val / total_val\n",
        "\n",
        "        # Store results for plotting later\n",
        "        train_losses.append(avg_train_loss)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Log the progress\n",
        "        log_msg = (f\"Epoch [{epoch+1}/{config.NUM_EPOCHS}] \"\n",
        "                   f\"- Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% \"\n",
        "                   f\"- Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
        "        print(log_msg)\n",
        "\n",
        "        # Append logs to a file\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(log_msg + \"\\n\")\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0  # Reset counter since we improved\n",
        "            torch.save(model.state_dict(), os.path.join(save_path, f\"{model_name}_best.pth\"))\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            # Early stopping if no improvement for 'patience' epochs\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1} due to no improvement in validation loss.\")\n",
        "                break\n",
        "\n",
        "        # Save model checkpoint periodically\n",
        "        if (epoch + 1) % config.SAVE_EVERY == 0:\n",
        "            torch.save(model.state_dict(), os.path.join(save_path, f\"{model_name}_epoch{epoch+1}.pth\"))\n",
        "\n",
        "    # After training, plot losses and accuracies\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs, train_losses, 'b-', label='Train Loss')\n",
        "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Train & Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs, train_accuracies, 'b-', label='Train Accuracy')\n",
        "    plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Train & Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    # Save the plot image in the same directory\n",
        "    plt.savefig(os.path.join(save_path, \"training_plots.png\"))\n",
        "    plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjkcXEMT3uot"
      },
      "source": [
        "#### Test Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO5kUIq9GMDf"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader, class_names, save_path):\n",
        "    # Move model to the device (GPU/CPU)\n",
        "    model.to(config.DEVICE)\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout, batchnorm, etc.)\n",
        "\n",
        "    predictions = []  # To store predicted class indices\n",
        "    real_labels = []  # To store true class indices\n",
        "    correct = 0       # Count of correct predictions\n",
        "    total = 0         # Total samples processed\n",
        "\n",
        "    # Directory to save classified images (one frame per video)\n",
        "    image_save_dir = os.path.join(save_path, \"classified_images\")\n",
        "    os.makedirs(image_save_dir, exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():  # No gradient computation needed during testing\n",
        "        for idx, (videos, labels) in enumerate(tqdm(test_loader)):\n",
        "            videos = videos.to(config.DEVICE)\n",
        "            labels = labels.to(config.DEVICE)\n",
        "\n",
        "            outputs = model(videos)               # Forward pass\n",
        "            preds = torch.argmax(outputs, dim=1) # Predicted classes\n",
        "            true_labels = torch.argmax(labels, dim=1)  # True classes\n",
        "\n",
        "            # Store predictions and true labels for all batches\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            real_labels.extend(true_labels.cpu().numpy())\n",
        "\n",
        "            # Count correct predictions for accuracy\n",
        "            correct += (preds == true_labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            # Save one image frame per video with predicted and real label in filename\n",
        "            for i in range(videos.size(0)):\n",
        "                pred_label = class_names[preds[i]]\n",
        "                real_label = class_names[true_labels[i]]\n",
        "                img_name = f\"{pred_label}_{real_label}_{idx}_{i}.jpg\"\n",
        "                img_path = os.path.join(image_save_dir, img_name)\n",
        "\n",
        "                # Extract first frame (channel 0) of the video tensor for saving\n",
        "                frame = videos[i][0].unsqueeze(0)  # Add channel dim for saving\n",
        "                vutils.save_image(frame, img_path)\n",
        "\n",
        "    # Save predictions and true labels to CSV for analysis\n",
        "    result_path = os.path.join(save_path, \"test_results.csv\")\n",
        "    df = pd.DataFrame({\n",
        "        \"Video_Index\": list(range(len(predictions))),\n",
        "        \"Predicted_Class\": [class_names[p] for p in predictions],\n",
        "        \"Real_Class\": [class_names[r] for r in real_labels]\n",
        "    })\n",
        "    df.to_csv(result_path, index=False)\n",
        "\n",
        "    # Calculate overall accuracy and save it to a text file\n",
        "    accuracy = 100 * correct / total\n",
        "    acc_file = os.path.join(save_path, \"test_accuracy.txt\")\n",
        "    with open(acc_file, \"w\") as f:\n",
        "        f.write(f\"Test Accuracy: {accuracy:.2f}%\\n\")\n",
        "\n",
        "    # Print summary to console\n",
        "    print(f\"\\nTest accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"Test results saved to: {result_path}\")\n",
        "    print(f\"Images saved in: {image_save_dir}\")\n",
        "    print(f\"Accuracy saved in: {acc_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzW1sRq33z7w"
      },
      "source": [
        "#### Run Training and Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to8e2g5pGlIg"
      },
      "outputs": [],
      "source": [
        "def run_training_and_testing(models, train_loader, val_loader, test_loader, num_classes, testing=True):\n",
        "    # Ensure models is a list\n",
        "    if isinstance(models, str):\n",
        "        models = [models]\n",
        "\n",
        "    for model_name in models:\n",
        "        print(f\"\\n=== Training and Testing Model: {model_name} ===\")\n",
        "\n",
        "        # Instantiate model, optimizer, and loss function\n",
        "        model = get_model_instance(model_name, num_classes)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Train and validate the model\n",
        "        train_and_validate(model, train_loader, val_loader, optimizer, criterion, model_name=model_name)\n",
        "\n",
        "        if testing:\n",
        "            # Prepare save directory for test results\n",
        "            save_path = os.path.join(config.SAVE_PATH, model_name)\n",
        "            os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "            # Get class names (sorted) from training data folder limited to selected classes\n",
        "            class_names_dir = os.path.join(config.DATASET_PATH, \"train\")\n",
        "            class_names = sorted([d for d in os.listdir(class_names_dir) if os.path.isdir(os.path.join(class_names_dir, d))])[:config.SELECTED_CLASSES]\n",
        "\n",
        "            # Test the trained model\n",
        "            test_model(model, test_loader, class_names, save_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80LOGB-L3475"
      },
      "source": [
        "#### Run Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1p8K5KY6kA8e",
        "outputId": "1951e95d-2247-4413-d8f3-d3041b45d061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Training and Testing Model: SingleFrameCNN ===\n",
            "Epoch [1/100] - Train Loss: 2.1669, Train Acc: 20.62% - Val Loss: 2.0146, Val Acc: 25.81%\n",
            "Epoch [2/100] - Train Loss: 1.7738, Train Acc: 36.18% - Val Loss: 1.6115, Val Acc: 48.39%\n",
            "Epoch [3/100] - Train Loss: 1.4977, Train Acc: 47.64% - Val Loss: 1.4404, Val Acc: 48.39%\n",
            "Epoch [4/100] - Train Loss: 1.3337, Train Acc: 53.24% - Val Loss: 1.2473, Val Acc: 50.54%\n",
            "Epoch [5/100] - Train Loss: 1.1814, Train Acc: 57.42% - Val Loss: 1.1200, Val Acc: 52.69%\n",
            "Epoch [6/100] - Train Loss: 1.0631, Train Acc: 61.42% - Val Loss: 0.9609, Val Acc: 68.28%\n",
            "Epoch [7/100] - Train Loss: 0.9591, Train Acc: 65.78% - Val Loss: 0.9664, Val Acc: 61.83%\n",
            "Epoch [8/100] - Train Loss: 0.8848, Train Acc: 66.93% - Val Loss: 0.8723, Val Acc: 67.74%\n",
            "Epoch [9/100] - Train Loss: 0.8181, Train Acc: 70.22% - Val Loss: 0.7776, Val Acc: 67.74%\n",
            "Epoch [10/100] - Train Loss: 0.7518, Train Acc: 71.73% - Val Loss: 0.6928, Val Acc: 73.12%\n",
            "Epoch [11/100] - Train Loss: 0.6948, Train Acc: 74.58% - Val Loss: 0.6999, Val Acc: 70.43%\n",
            "Epoch [12/100] - Train Loss: 0.6540, Train Acc: 77.07% - Val Loss: 0.6570, Val Acc: 74.73%\n",
            "Epoch [13/100] - Train Loss: 0.5972, Train Acc: 78.76% - Val Loss: 0.5962, Val Acc: 74.73%\n",
            "Epoch [14/100] - Train Loss: 0.5506, Train Acc: 79.47% - Val Loss: 0.6132, Val Acc: 73.12%\n",
            "Epoch [15/100] - Train Loss: 0.5196, Train Acc: 80.18% - Val Loss: 0.6086, Val Acc: 79.57%\n",
            "Epoch [16/100] - Train Loss: 0.4876, Train Acc: 81.16% - Val Loss: 0.5564, Val Acc: 74.19%\n",
            "Epoch [17/100] - Train Loss: 0.4682, Train Acc: 83.47% - Val Loss: 0.6069, Val Acc: 76.88%\n",
            "Epoch [18/100] - Train Loss: 0.4529, Train Acc: 81.96% - Val Loss: 0.5251, Val Acc: 77.42%\n",
            "Epoch [19/100] - Train Loss: 0.4015, Train Acc: 84.62% - Val Loss: 0.5133, Val Acc: 77.42%\n",
            "Epoch [20/100] - Train Loss: 0.3689, Train Acc: 85.96% - Val Loss: 0.5528, Val Acc: 75.27%\n",
            "Epoch [21/100] - Train Loss: 0.3543, Train Acc: 87.20% - Val Loss: 0.5017, Val Acc: 75.27%\n",
            "Epoch [22/100] - Train Loss: 0.3336, Train Acc: 86.22% - Val Loss: 0.5451, Val Acc: 76.88%\n",
            "Epoch [23/100] - Train Loss: 0.3188, Train Acc: 87.82% - Val Loss: 0.4833, Val Acc: 77.42%\n",
            "Epoch [24/100] - Train Loss: 0.3098, Train Acc: 87.82% - Val Loss: 0.4895, Val Acc: 80.11%\n",
            "Epoch [25/100] - Train Loss: 0.2882, Train Acc: 88.80% - Val Loss: 0.4914, Val Acc: 76.88%\n",
            "Epoch [26/100] - Train Loss: 0.2760, Train Acc: 88.80% - Val Loss: 0.4592, Val Acc: 76.34%\n",
            "Epoch [27/100] - Train Loss: 0.2533, Train Acc: 90.13% - Val Loss: 0.4916, Val Acc: 81.72%\n",
            "Epoch [28/100] - Train Loss: 0.2502, Train Acc: 89.16% - Val Loss: 0.4940, Val Acc: 75.27%\n",
            "Epoch [29/100] - Train Loss: 0.2408, Train Acc: 90.58% - Val Loss: 0.5065, Val Acc: 79.03%\n",
            "Epoch [30/100] - Train Loss: 0.2233, Train Acc: 89.87% - Val Loss: 0.5840, Val Acc: 73.12%\n",
            "Epoch [31/100] - Train Loss: 0.2186, Train Acc: 90.13% - Val Loss: 0.4722, Val Acc: 79.03%\n",
            "Early stopping at epoch 31 due to no improvement in validation loss.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 49/49 [00:02<00:00, 20.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test accuracy: 80.41%\n",
            "Test results saved to: /content/drive/MyDrive/MSAI/DeepLearning/assignment_07/SingleFrameCNN/test_results.csv\n",
            "Images saved in: /content/drive/MyDrive/MSAI/DeepLearning/assignment_07/SingleFrameCNN/classified_images\n",
            "Accuracy saved in: /content/drive/MyDrive/MSAI/DeepLearning/assignment_07/SingleFrameCNN/test_accuracy.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "config.MODEL = [\"SingleFrameCNN\"] # \"SingleFrameCNN\", \"C3DLite\", \"CNNLSTM\", \"SlowFast\"\n",
        "run_training_and_testing(config.MODEL, train_loader, val_loader, test_loader, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKpxcORzv3FO"
      },
      "source": [
        "## Detect Live Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBJ0ghjyv5c5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# ---------------- CONFIG ---------------- #\n",
        "class Config:\n",
        "    IMAGE_SIZE = 32\n",
        "    FRAME_RATE = 30\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    CLASS_NAMES = ['ApplyEyeMakeup',\n",
        "                    'ApplyLipstick',\n",
        "                    'Archery',\n",
        "                    'BabyCrawling',\n",
        "                    'BalanceBeam',\n",
        "                    'BandMarching',\n",
        "                    'BaseballPitch',\n",
        "                    'Basketball',\n",
        "                    'BasketballDunk',\n",
        "                    'BenchPress']  # Replace with real class names\n",
        "\n",
        "# ---------------- MODEL LOADER ---------------- #\n",
        "def get_model_instance(model_name, num_classes):\n",
        "    if model_name == \"SingleFrameCNN\":\n",
        "        return SingleFrameCNN(num_classes=num_classes)\n",
        "    elif model_name == \"C3DLite\":\n",
        "        return C3DLite(num_classes=num_classes)\n",
        "    elif model_name == \"CNNLSTM\":\n",
        "        return CNNLSTM(num_classes=num_classes)\n",
        "    elif model_name == \"SlowFast\":\n",
        "        return SlowFast(num_classes=num_classes)\n",
        "    else:\n",
        "        raise ValueError(f\"Model '{model_name}' is not supported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkZIx0rMrUW7"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path, model_name, num_classes):\n",
        "    model = get_model_instance(model_name, num_classes)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=Config.DEVICE))\n",
        "    model.to(Config.DEVICE)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# ---------------- FRAME PROCESSING ---------------- #\n",
        "def process_frame(frame, model):\n",
        "    resized = cv2.resize(frame, (Config.IMAGE_SIZE, Config.IMAGE_SIZE))\n",
        "    normalized = resized.astype(np.float32) / 255.0\n",
        "    chw = np.transpose(normalized, (2, 0, 1))\n",
        "    input_tensor = torch.tensor(chw, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(Config.DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        pred_idx = torch.argmax(output, dim=1).item()\n",
        "\n",
        "    return pred_idx, input_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH2nwlUNrVdH"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# ---------------- VIDEO PROCESSING ---------------- #\n",
        "def process_video(video_path, model, output_path=None, show=False, predict=True):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise IOError(f\"Cannot open video: {video_path}\")\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_skip = max(int(fps / Config.FRAME_RATE), 1)\n",
        "\n",
        "    # Prepare video writer if saving\n",
        "    out_writer = None\n",
        "    if output_path:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        out_writer = cv2.VideoWriter(output_path, fourcc, Config.FRAME_RATE, (width, height))\n",
        "\n",
        "    print(f\"Processing: {os.path.basename(video_path)}\")\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        current_frame = int(cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "        if current_frame % frame_skip != 0:\n",
        "            continue\n",
        "\n",
        "        display_frame = frame.copy()\n",
        "\n",
        "        if predict:\n",
        "            if len(frame.shape) == 2 or frame.shape[2] == 1:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "            pred_idx, _ = process_frame(frame, model)\n",
        "            print(\"Index is :\",pred_idx)\n",
        "            label = Config.CLASS_NAMES[pred_idx]\n",
        "\n",
        "            cv2.putText(display_frame, f\"Predicted: {label}\", (10, 30),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
        "\n",
        "        if show:\n",
        "            clear_output(wait=True)\n",
        "            disp_img = cv2.resize(display_frame, (640, 480))\n",
        "            disp_img = cv2.cvtColor(disp_img, cv2.COLOR_BGR2RGB)\n",
        "            display(Image.fromarray(disp_img))\n",
        "            time.sleep(1 / Config.FRAME_RATE)\n",
        "\n",
        "        if out_writer:\n",
        "            out_writer.write(display_frame)\n",
        "\n",
        "    cap.release()\n",
        "    if out_writer:\n",
        "        out_writer.release()\n",
        "\n",
        "    print(f\"Finished processing: {os.path.basename(video_path)}\")\n",
        "\n",
        "# ---------------- FOLDER SUPPORT ---------------- #\n",
        "def process_path(input_path, model, output_dir=None, show=False, predict=True):\n",
        "    if os.path.isfile(input_path):\n",
        "        out_path = os.path.join(output_dir, os.path.basename(input_path)) if output_dir else None\n",
        "        process_video(input_path, model, output_path=out_path, show=show, predict=predict)\n",
        "    elif os.path.isdir(input_path):\n",
        "        video_files = [f for f in os.listdir(input_path) if f.endswith(('.mp4', '.avi'))]\n",
        "        video_files = random.sample(video_files, min(1, len(video_files)))\n",
        "        for vid in video_files:\n",
        "            full_path = os.path.join(input_path, vid)\n",
        "            out_path = os.path.join(output_dir, vid) if output_dir else None\n",
        "            process_video(full_path, model, output_path=out_path, show=show, predict=predict)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid input path. Must be a video file or directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khkVkp1bAjf_"
      },
      "outputs": [],
      "source": [
        "# Customize here\n",
        "model_name = \"SingleFrameCNN\"\n",
        "model_path = '/SingleFrameCNN/SingleFrameCNN_best.pth'\n",
        "config = Config()\n",
        "for i in config.CLASS_NAMES:\n",
        "  input_path = f'/ucf101_data/test/{i}/'\n",
        "  output_dir = '/SingleFrameCNN/output_videos'  # or None\n",
        "  show_video = False\n",
        "  predict_video = True\n",
        "\n",
        "  print(\"I path :\", input_path)\n",
        "\n",
        "  os.makedirs(output_dir, exist_ok=True) if output_dir else None\n",
        "  model = load_model(model_path, model_name, num_classes=10)\n",
        "\n",
        "  process_path(input_path, model, output_dir=output_dir, show=show_video, predict=predict_video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhHp1riftG7t"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "from glob import glob\n",
        "from IPython.display import display, clear_output\n",
        "from PIL import Image as PILImage\n",
        "import time\n",
        "\n",
        "def combine_and_play_videos(input_folder, download=False, save_path=None, show_video=True):\n",
        "    # Get all video files\n",
        "    video_extensions = ('*.avi', '*.mp4', '*.mov', '*.mkv')\n",
        "    video_files = []\n",
        "    for ext in video_extensions:\n",
        "        video_files.extend(glob(os.path.join(input_folder, ext)))\n",
        "\n",
        "    if not video_files:\n",
        "        print(\"No video files found in the specified folder.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(video_files)} videos. Combining and playing...\")\n",
        "\n",
        "    # Read properties from first video\n",
        "    cap0 = cv2.VideoCapture(video_files[0])\n",
        "    width  = int(cap0.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap0.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap0.get(cv2.CAP_PROP_FPS)\n",
        "    cap0.release()\n",
        "\n",
        "    if download:\n",
        "        if save_path is None:\n",
        "            raise ValueError(\"Save path must be specified when download is True.\")\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "        out_writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))\n",
        "\n",
        "    for video_path in video_files:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if download:\n",
        "                out_writer.write(frame)\n",
        "\n",
        "            if show_video:\n",
        "                resized = cv2.resize(frame, (640, 480))\n",
        "                rgb_img = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
        "                pil_img = PILImage.fromarray(rgb_img)\n",
        "\n",
        "                clear_output(wait=True)  # Clear previous frame\n",
        "                display(pil_img)         # Show current frame\n",
        "                time.sleep(1 / fps)\n",
        "\n",
        "        cap.release()\n",
        "\n",
        "    if download:\n",
        "        out_writer.release()\n",
        "        print(f\"Video saved to {save_path}\")\n",
        "\n",
        "    print(\"All videos played and processed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PFu3t84CtL9G"
      },
      "outputs": [],
      "source": [
        "save_pth = \"SingleFrameCNN/output_30.mp4\"\n",
        "combine_and_play_videos(output_dir, download=True, save_path=save_pth, show_video=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}