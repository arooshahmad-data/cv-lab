{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC3gyg5_eD04"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6RZ3ZNHdrNM",
        "outputId": "c94271d7-a879-40dc-a211-d45bf065b3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive is already mounted.\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive, if not already mounted\n",
        "\n",
        "if not os.path.ismount(\"/content/drive\"):\n",
        "    drive.mount(\"/content/drive\")\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjRxLSOt3pH6"
      },
      "source": [
        "## Download and Extract Dataset\n",
        "\n",
        "**IMPORTANT**:\n",
        "\n",
        "Only run the below cells if you want to download the dataset.\n",
        "\n",
        "If dataset is already downloaded do not run the below cells.\n",
        "\n",
        "[Dataset Link](https://www.kaggle.com/datasets/karandeep98/real-life-violence-and-nonviolence-data)\n",
        "\n",
        "[Dataset Link 2](https://www.kaggle.com/datasets/mohamedmustafa/real-life-violence-situations-dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgtKw_W5u2N0",
        "outputId": "7f621f20-6d1d-438e-e76b-d3d09b6bbc0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MSAI/DeepLearning/FinalProject\n"
          ]
        }
      ],
      "source": [
        "# Change Working Directory\n",
        "%cd /content/drive/MyDrive/MSAI/DeepLearning/FinalProject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwdbHDt02Cuq"
      },
      "outputs": [],
      "source": [
        "DOWNLOAD_DATASET = False\n",
        "if DOWNLOAD_DATASET:\n",
        "  from google.colab import files\n",
        "  files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sydqf-Zc2IJT"
      },
      "outputs": [],
      "source": [
        "# Make directory to download kaggle dataset\n",
        "\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFTapmXI2Lua",
        "outputId": "cc2a1a62-d59e-49c2-d7e6-f3a05b570fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/karandeep98/real-life-violence-and-nonviolence-data\n",
            "License(s): unknown\n",
            "User cancelled operation\n"
          ]
        }
      ],
      "source": [
        "# Download\n",
        "\n",
        "# !kaggle datasets download -d karandeep98/real-life-violence-and-nonviolence-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kp7HVRdvGkR"
      },
      "outputs": [],
      "source": [
        "#Download only if the the data is not already present\n",
        "\n",
        "if DOWNLOAD_DATASET:\n",
        "  import zipfile\n",
        "  import os\n",
        "\n",
        "  dataset_zip = \"real-life-violence-and-nonviolence-data.zip\"\n",
        "  extract_path = \"real-life-violence-and-nonviolence-data\"\n",
        "\n",
        "  with zipfile.ZipFile(dataset_zip, 'r') as zip_ref:\n",
        "      zip_ref.extractall(extract_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1LlSKfPcGdM"
      },
      "source": [
        "## Dataset Pre-Processing\n",
        "\n",
        "There are two data folders\n",
        "\n",
        "- violence\n",
        "- non-violence\n",
        "\n",
        "###### **Note:**\n",
        "We donot have videos as mp4 but frames as jpg images so for an image all frames are extracted.\n",
        "Each folder contains the frames of videos, each video has specific id like, V1000.mp4_frame1.jpg, V1000.mp4_frame2.jpg, V1000.mp4_frame3.jpg so on ...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmGjXFMVea6O"
      },
      "outputs": [],
      "source": [
        "# Change working directory to FinalProject Folder\n",
        "\n",
        "%cd /content/drive/MyDrive/violence_detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ks1lP3R52oFM"
      },
      "outputs": [],
      "source": [
        "violence_dir = \"real-life-violence-and-nonviolence-data/violence_dataset/violence\"\n",
        "non_violence_dir = \"real-life-violence-and-nonviolence-data/violence_dataset/non_violence\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGjVLrjk4HGz",
        "outputId": "4e671776-686a-4950-b236-73b18e1b3de8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5832"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_len = len(os.listdir(violence_dir))\n",
        "data_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge_hRQ51qHrM"
      },
      "outputs": [],
      "source": [
        "# split dataset function (helping-source : Hands on Macine Learning second edition)\n",
        "def split_dataset(samples, test_size=0.2, seed=42):\n",
        "    train_samples, test_samples = train_test_split(samples, test_size=test_size, random_state=seed)\n",
        "    return train_samples, test_samples\n",
        "\n",
        "# transform image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, samples, num_frames=16, transform=None, _3DCNN = False):\n",
        "        self.samples = samples\n",
        "        self.num_frames = num_frames\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "            Dunder method to get item (video, label) pair\n",
        "        \"\"\"\n",
        "        frame_paths, label = self.samples[index]\n",
        "        selected_frames = frame_paths[:self.num_frames]\n",
        "        if len(selected_frames) < self.num_frames:\n",
        "            selected_frames += [selected_frames[-1]] * (self.num_frames - len(selected_frames))\n",
        "\n",
        "        frames = []\n",
        "        for frame_path in selected_frames:\n",
        "            img = Image.open(frame_path).convert(\"RGB\") # convert image to RGB\n",
        "            if self.transform:\n",
        "                img = self.transform(img) # apply transform\n",
        "            frames.append(img)\n",
        "\n",
        "        video_tensor = torch.stack(frames)  # (T, C, H, W)\n",
        "\n",
        "        if _3DCNN:\n",
        "          # Convert to 3D tensor for 3DCNN, shape: [B, C=3, T, H, W]\n",
        "          video_tensor = video_tensor.permute(0, 2, 1, 3, 4)  # From [B, T, C, H, W] to [B, C, T, H, W]\n",
        "        return video_tensor, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPDp23xYvJhO"
      },
      "outputs": [],
      "source": [
        "def build_samples(root_dir):\n",
        "    \"\"\"\n",
        "      Build samples\n",
        "      video: list of frames appended as numeric arrays\n",
        "      label: numeric representation (0: violence, 1: no-violence)\n",
        "    \"\"\"\n",
        "    samples = []\n",
        "    for class_name in [\"violence\", \"non_violence\"]:\n",
        "        label = 0 if class_name == \"violence\" else 1\n",
        "        class_path = os.path.join(root_dir, class_name)\n",
        "\n",
        "        # skip if directory not found\n",
        "        if not os.path.exists(class_path):\n",
        "            print(f\"Directory does not exist: {class_path}\")\n",
        "            continue\n",
        "\n",
        "        videos = defaultdict(list)\n",
        "        for file in os.listdir(class_path):\n",
        "            if file.endswith(\".jpg\"):\n",
        "                video_id = \"_\".join(file.split(\"_\")[:-1])  # V1000.mp4\n",
        "                videos[video_id].append(os.path.join(class_path, file))\n",
        "\n",
        "        for video_id, frame_list in videos.items():\n",
        "            frame_list.sort()\n",
        "            samples.append((frame_list, label))\n",
        "\n",
        "    return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzPaEjvIm62c"
      },
      "outputs": [],
      "source": [
        "\n",
        "root_dir = \"real-life-violence-and-nonviolence-data/violence_dataset\"\n",
        "\n",
        "# Build all samples first\n",
        "all_samples = build_samples(root_dir)\n",
        "\n",
        "# Split into train (80%) and temp (20%)\n",
        "train_samples, temp_samples = train_test_split(all_samples, test_size=0.2, random_state=42, stratify=[label for _, label in all_samples])\n",
        "\n",
        "# Split temp set into validation (10%) and test (10%)\n",
        "val_samples, test_samples = train_test_split(temp_samples, test_size=0.5, random_state=42, stratify=[label for _, label in temp_samples])\n",
        "\n",
        "# 3DCNN requires shape [B, C=3, T, H, W]\n",
        "if model == \"3DCNN\":\n",
        "  _3DCNN = True\n",
        "\n",
        "train_dataset = VideoDataset(train_samples, num_frames=16, transform=transform, _3DCNN = _3DCNN)\n",
        "val_dataset   = VideoDataset(val_samples, num_frames=16, transform=transform, _3DCNN = _3DCNN)\n",
        "test_dataset  = VideoDataset(test_samples, num_frames=16, transform=transform, _3DCNN = _3DCNN)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz0-XUkGwF6m"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1yK4Wsy36ew"
      },
      "outputs": [],
      "source": [
        "class CNNVideoClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2, pretrained=True, freeze_cnn=True):\n",
        "        super(CNNVideoClassifier, self).__init__()\n",
        "\n",
        "        # Load pretrained ResNet18\n",
        "        resnet = models.resnet18(pretrained=pretrained)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n",
        "        self.feature_dim = 512  # Final feature size from ResNet18\n",
        "\n",
        "        if freeze_cnn:\n",
        "            for param in self.cnn.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.feature_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: [B, T, C, H, W]\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)  # Merge batch and time\n",
        "\n",
        "        with torch.no_grad():  # CNN frozen\n",
        "            features = self.cnn(x)  # [B*T, 512, 1, 1]\n",
        "        features = features.view(B, T, self.feature_dim)  # [B, T, 512]\n",
        "\n",
        "        video_features = features.mean(dim=1)  # Average over frames (T) -> [B, 512]\n",
        "\n",
        "        out = self.classifier(video_features)  # [B, num_classes]\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDBDAAHnuVyr"
      },
      "outputs": [],
      "source": [
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, lstm_layers=1, num_classes=2):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # Pretrained CNN (ResNet18)\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n",
        "        self.feature_dim = 512  # ResNet18 final feature size\n",
        "\n",
        "        # Freeze CNN (optional)\n",
        "        for param in self.cnn.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(input_size=self.feature_dim, hidden_size=hidden_dim,\n",
        "                            num_layers=lstm_layers, batch_first=True)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):  # x: [B, T, C, H, W]\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "\n",
        "        with torch.no_grad():  # CNN is frozen\n",
        "            features = self.cnn(x)  # Output: (B*T, 512, 1, 1)\n",
        "\n",
        "        features = features.view(B, T, self.feature_dim)  # Reshape to (B, T, 512)\n",
        "        lstm_out, _ = self.lstm(features)  # (B, T, hidden_dim)\n",
        "        final_output = lstm_out[:, -1, :]  # Take last timestep\n",
        "        out = self.classifier(final_output)  # (B, num_classes)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class C3DModel(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(C3DModel, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv3d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2)),  # Time preserved\n",
        "\n",
        "            nn.Conv3d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
        "\n",
        "            nn.Conv3d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2)),\n",
        "\n",
        "            nn.Conv3d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.AdaptiveMaxPool3d((1, 1, 1))  # Output: (B, 512, 1, 1, 1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):  # x: [B, C=3, T, H, W]\n",
        "        x = self.features(x)  # -> [B, 512, 1, 1, 1]\n",
        "        out = self.classifier(x)  # -> [B, num_classes]\n",
        "        return out"
      ],
      "metadata": {
        "id": "QbUYdlLjMEAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoOFDaLYwIEp"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8F1lBOEGwHM-",
        "collapsed": true,
        "outputId": "15db8e75-abc6-4c6b-8d70-0ea2d17c69d6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 106MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Starting training from scratch.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [1:14:31<00:00, 11.18s/it, acc=0.701, loss=0.528]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50] Completed â€” Train Loss: 234.3320, Train Accuracy: 0.7006\n",
            "Validation Loss: 21.0927, Validation Accuracy: 0.8750\n",
            "Best model saved with validation accuracy: 0.8750\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [52:06<00:00,  7.82s/it, acc=0.783, loss=0.502]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/50] Completed â€” Train Loss: 194.7992, Train Accuracy: 0.7831\n",
            "Validation Loss: 16.4624, Validation Accuracy: 0.8700\n",
            "No improvement in validation accuracy for 1 epoch(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [3/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [52:01<00:00,  7.80s/it, acc=0.805, loss=0.235]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/50] Completed â€” Train Loss: 175.2940, Train Accuracy: 0.8050\n",
            "Validation Loss: 14.1340, Validation Accuracy: 0.8900\n",
            "Best model saved with validation accuracy: 0.8900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [4/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [51:55<00:00,  7.79s/it, acc=0.796, loss=0.133]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/50] Completed â€” Train Loss: 179.3780, Train Accuracy: 0.7963\n",
            "Validation Loss: 13.4528, Validation Accuracy: 0.9100\n",
            "Best model saved with validation accuracy: 0.9100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [5/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [51:46<00:00,  7.77s/it, acc=0.82, loss=0.95]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/50] Completed â€” Train Loss: 165.8595, Train Accuracy: 0.8200\n",
            "Validation Loss: 13.8668, Validation Accuracy: 0.8750\n",
            "No improvement in validation accuracy for 1 epoch(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [6/50]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [51:53<00:00,  7.78s/it, acc=0.819, loss=0.742]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/50] Completed â€” Train Loss: 164.1229, Train Accuracy: 0.8187\n",
            "Validation Loss: 12.5272, Validation Accuracy: 0.8850\n",
            "No improvement in validation accuracy for 2 epoch(s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [7/50]:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 362/400 [46:59<04:55,  7.77s/it, acc=0.832, loss=0.569]"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "EPOCHS = 50\n",
        "PATIENCE = 3\n",
        "USE_PRETRAINED_WEIGHTS = False  #Set to True to load and continue from saved weights\n",
        "output_dir = \"models/\"\n",
        "model_path = os.path.join(output_dir, \"best_model_cnn.pth\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model setup\n",
        "# model = CNNLSTMModel().to(device)\n",
        "model = CNNVideoClassifier().to(device)\n",
        "\n",
        "if USE_PRETRAINED_WEIGHTS and os.path.exists(model_path):\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    print(f\"Loaded pre-trained weights from: {model_path}\")\n",
        "else:\n",
        "    print(f\"ðŸš€ Starting training from scratch.\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = CrossEntropyLoss() # Binary Cross Entropy is better\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Early stopping variables\n",
        "best_val_acc = 0.0\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0 # number of correct preds (training)\n",
        "    total = 0 # number of total samples (training)\n",
        "\n",
        "    train_loop = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
        "\n",
        "    for step, (videos, labels) in train_loop:\n",
        "        videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(videos)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad() # get gradients to zero\n",
        "        loss.backward() # perform backward propogation\n",
        "        optimizer.step() # optimize\n",
        "\n",
        "        # Calculate loss\n",
        "        total_loss += loss.item()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        train_loop.set_postfix(loss=loss.item(), acc=correct / total)\n",
        "\n",
        "    train_acc = correct / total\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Completed â€” Train Loss: {total_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0 # number of correct preds (validation)\n",
        "    val_total = 0 # number of total samples (validation)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for videos, labels in val_loader:\n",
        "            videos, labels = videos.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Early stopping and checkpointing\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Best model saved with validation accuracy: {best_val_acc:.4f}\")\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f\"No improvement in validation accuracy for {epochs_no_improve} epoch(s)\")\n",
        "\n",
        "    if epochs_no_improve >= PATIENCE:\n",
        "        print(\"Early stopping triggered due to no improvement in validation accuracy.\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhsfR-1AwmcI"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"models/best_model_clstm.pth\")"
      ],
      "metadata": {
        "id": "hQ1Vtva-HoZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZkfYIqCwiWS"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt1numgdwpzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1e1c181a-46d2-4bf3-a3bd-bdfc749f7e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 277MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNLSTMModel(\n",
              "  (cnn): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (lstm): LSTM(512, 256, batch_first=True)\n",
              "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model setup\n",
        "model = CNNLSTMModel().to(device)\n",
        "# model = CNNVideoClassifier().to(device)\n",
        "# model = C3DModel().to(device)\n",
        "model.load_state_dict(torch.load(\"models/best_model_clstm.pth\"))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for videos, labels in test_loader:\n",
        "        videos, labels = videos.to(device), labels.to(device)\n",
        "        outputs = model(videos)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "test_acc = correct / total\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "7fpt7RYLHlXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test on Videos"
      ],
      "metadata": {
        "id": "9uUAY3-sI46X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Install dependencies --------------------\n",
        "# !pip install torchvision --quiet\n",
        "# !pip install matplotlib --quiet\n",
        "from google.colab import files"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_z2pK0wJI9ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============ Load model ============= #\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CNNLSTMModel()\n",
        "model.load_state_dict(torch.load(\"models/best_model_clstm.pth\", map_location=device))\n",
        "model.to(device).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "buWESAk2JPBS",
        "outputId": "4a042fb8-c4f5-403c-ba25-700c19a171f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNLSTMModel(\n",
              "  (cnn): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (4): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  )\n",
              "  (lstm): LSTM(512, 256, batch_first=True)\n",
              "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()  # Upload your .mp4 video\n",
        "video_path = list(uploaded.keys())[0]\n",
        "print(\"Uploaded:\", video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "3a2G08JQJIZY",
        "outputId": "ef39434e-6251-4fdd-de9b-f3c05db61be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e97b8657-3dd6-41d8-aba8-d0acf003c25d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e97b8657-3dd6-41d8-aba8-d0acf003c25d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving NV_combined_video.mp4 to NV_combined_video (1).mp4\n",
            "Uploaded: NV_combined_video (1).mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## ============= Extract Frames ============= #\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def extract_frames(video_path, num_frames=16):\n",
        "    \"\"\"\n",
        "      Extract Video Frames\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_idxs = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
        "    frames = []\n",
        "    idx_set = set(frame_idxs.tolist())\n",
        "    count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count in idx_set:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = transform(Image.fromarray(frame))\n",
        "            frames.append(frame)\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(\"No frames extracted.\")\n",
        "    while len(frames) < num_frames:\n",
        "        frames.append(frames[-1])  # Pad with last frame\n",
        "\n",
        "    video_tensor = torch.stack(frames)  # [T, C, H, W]\n",
        "    return video_tensor.unsqueeze(0)  # [1, T, C, H, W]\n"
      ],
      "metadata": {
        "id": "h3QYOMbLJTiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= Predict ============ #\n",
        "\n",
        "with torch.no_grad():\n",
        "    video_tensor = extract_frames(video_path).to(device)\n",
        "    output = model(video_tensor)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    label = \"ðŸŸ¥ Violent\" if predicted.item() == 0 else \"ðŸŸ© Non-Violent\"\n",
        "    print(f\"Prediction: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9wF4L9oJdoS",
        "outputId": "792e2b19-43bb-4c1b-dd6c-fa5f0f7e5dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: ðŸŸ© Non-Violent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit UI"
      ],
      "metadata": {
        "id": "1dmjphlr1oqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear Streamlit Cache\n",
        "import streamlit as st\n",
        "st.cache_resource.clear()"
      ],
      "metadata": {
        "id": "zNQXawdU1glq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### installations"
      ],
      "metadata": {
        "id": "2zZdzIVhpBW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit"
      ],
      "metadata": {
        "id": "t2LgWtQ75WXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok -q"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-hvf0wQy1l7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok ffmpeg-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SG3NfagY5c-t",
        "outputId": "ffcdcaba-1f80-4402-d104-7d9be747dc60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: ffmpeg-python in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.42.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set ngrok authentication token\n",
        "!ngrok config add-authtoken 2ycNlcF5rJDLaQ2EZTuU2ejmgzV_7m2fdgUujhzKoHtFRxJwc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5tyZKEm4jAE",
        "outputId": "cc8d627e-ab1e-44b0-bde9-72b3f2a5e96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### writing files"
      ],
      "metadata": {
        "id": "1RF8fy-73dLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "from model import CNNLSTMModel  # Replace if you're switching models\n",
        "\n",
        "# ---------- Page Config ----------\n",
        "st.set_page_config(page_title=\"Violence Detection AI\", page_icon=\"ðŸš¨\", layout=\"wide\")\n",
        "\n",
        "# ---------- Custom CSS ----------\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .title {\n",
        "        font-size: 2.5em;\n",
        "        font-weight: bold;\n",
        "        text-align: center;\n",
        "        color: #D7263D;\n",
        "        margin-bottom: 5px;\n",
        "    }\n",
        "    .subtitle {\n",
        "        text-align: center;\n",
        "        font-size: 1.1em;\n",
        "        margin-bottom: 30px;\n",
        "        color: #444;\n",
        "    }\n",
        "    .box {\n",
        "        border: 1px solid #ddd;\n",
        "        padding: 20px;\n",
        "        border-radius: 10px;\n",
        "        background-color: white;\n",
        "        box-shadow: 2px 2px 6px rgba(0,0,0,0.05);\n",
        "        margin-top: 10px;\n",
        "    }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ---------- Sidebar ----------\n",
        "# st.sidebar.image(\"https://cdn-icons-png.flaticon.com/512/2409/2409342.png\" width='100')\n",
        "st.sidebar.markdown(\"\"\"\n",
        "    <div style='text-align: center;'>\n",
        "        <img src='https://cdn-icons-png.flaticon.com/512/2409/2409342.png' width='100'/>\n",
        "    </div>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "st.sidebar.header(\"ðŸ”¬ Project Overview\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "**Model Architectures:**\n",
        "- CNN + LSTM (final model)\n",
        "- Simple ResNet18\n",
        "- 3D-CNN (video-level)\n",
        "\n",
        "**Dataset:**\n",
        "- Real-life Violence Dataset\n",
        "\n",
        "**Goal:**\n",
        "- Predict \"Violent\" or \"Non-Violent\" from video\n",
        "\n",
        "**Made by:**\n",
        "- Aroosh Ahmad\n",
        "- Violence Detecction\n",
        "\"\"\")\n",
        "\n",
        "# ---------- Connect Links ----------\n",
        "st.sidebar.markdown(\"### ðŸ¤ Connect with Me\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "[![LinkedIn](https://img.shields.io/badge/-LinkedIn-blue?style=flat-square&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/aroosh-ahmad/)\n",
        "[![GitHub](https://img.shields.io/badge/-GitHub-black?style=flat-square&logo=github&logoColor=white)](https://github.com/arooshahmad)\n",
        "[![Kaggle](https://img.shields.io/badge/-Kaggle-blue?style=flat-square&logo=kaggle&logoColor=white)](https://www.kaggle.com/arooshahmadds)\n",
        "\"\"\")\n",
        "\n",
        "model_state_dict_path = \"/content/drive/MyDrive/violence_detection/models/best_model_clstm.pth\"\n",
        "\n",
        "# ---------- Load Model ----------\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = CNNLSTMModel()\n",
        "    model.load_state_dict(torch.load(model_state_dict_path, map_location=\"cpu\"))\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# ---------- Transform ----------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def extract_frames(video_path, num_frames=16):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    frame_idxs = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
        "    frames = []\n",
        "\n",
        "    idx_set = set(frame_idxs.tolist())\n",
        "    count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count in idx_set:\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame = transform(Image.fromarray(frame))\n",
        "            frames.append(frame)\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(\"No frames extracted from video.\")\n",
        "\n",
        "    while len(frames) < num_frames:\n",
        "        frames.append(frames[-1])\n",
        "\n",
        "    video_tensor = torch.stack(frames)  # [T, C, H, W]\n",
        "    video_tensor = video_tensor.unsqueeze(0)  # [1, T, C, H, W]\n",
        "    return video_tensor\n",
        "\n",
        "\n",
        "# ---------- App Title ----------\n",
        "st.markdown('<div class=\"title\">ðŸŽ¥ Real-Time Violence Detection</div>', unsafe_allow_html=True)\n",
        "st.markdown('<div class=\"subtitle\">Upload a short video and detect violence using AI-powered deep learning models.</div>', unsafe_allow_html=True)\n",
        "\n",
        "# ---------- Upload Form ----------\n",
        "with st.container():\n",
        "    with st.form(key=\"input_form\"):\n",
        "        st.markdown('<div class=\"box\">', unsafe_allow_html=True)\n",
        "        uploaded_video = st.file_uploader(\"ðŸ“¤ Upload a Video File\", type=[\"mp4\", \"mov\", \"avi\"])\n",
        "        submit_btn = st.form_submit_button(\"ðŸ” Analyze Video\")\n",
        "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "# ---------- Prediction ----------\n",
        "if uploaded_video:\n",
        "    st.video(uploaded_video)\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:\n",
        "        tmp_file.write(uploaded_video.read())\n",
        "        tmp_path = tmp_file.name\n",
        "\n",
        "    if submit_btn:\n",
        "        with st.spinner(\"â³ Processing...\"):\n",
        "            video_tensor = extract_frames(tmp_path).to(device)\n",
        "            with torch.no_grad():\n",
        "                output = model(video_tensor)\n",
        "                _, predicted = torch.max(output, 1)\n",
        "                label = \"ðŸŸ¥ Violent\" if predicted.item() == 0 else \"ðŸŸ© Non-Violent\"\n",
        "\n",
        "        st.markdown(f\"\"\"<div class=\"box\" style=\"text-align: center; font-size: 1.3em;\">\n",
        "            ðŸ”Ž <strong>Prediction:</strong> {label}\n",
        "        </div>\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# ---------- Expander ----------\n",
        "with st.expander(\"ðŸ“Š Architecture Comparison & Details\"):\n",
        "    st.markdown(\"\"\"\n",
        "| Model           | Description                                             | Pros                        | Accuracy |\n",
        "|----------------|---------------------------------------------------------|-----------------------------|----------|\n",
        "| **CNN + LSTM** | ResNet18 for spatial + LSTM for temporal features       | Best temporal reasoning     | â­ **92%** |\n",
        "| Simple ResNet  | ResNet18 classifier without sequence modeling           | Lightweight, fast           | ~78%     |\n",
        "| **3D CNN**     | 3D convolutions across time dimension                   | High video context capture  | ~85%     |\n",
        "\n",
        "- **Frame Count**: 16 evenly sampled\n",
        "- **Input Size**: 224x224 (resized)\n",
        "- **Prediction**: Binary (Violent / Non-Violent)\n",
        "\n",
        "ðŸ”— **Dataset**: [Real-life Violence Dataset on Kaggle](https://www.kaggle.com/datasets/karandeep98/real-life-violence-and-nonviolence-data)\n",
        "\n",
        "> ðŸš§ This system is a research prototype and not meant for production surveillance yet.\n",
        "    \"\"\")\n",
        "\n",
        "# ---------- Footer ----------\n",
        "st.markdown(\"\"\"\n",
        "---\n",
        "<center>\n",
        "Made with â¤ï¸ by <b>Aroosh Ahmad</b> â€¢ Violence Detection Project â€¢ 2025<br><br>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/arooshahmad-data/\" target=\"_blank\">\n",
        "<img src=\"https://img.shields.io/badge/LinkedIn-blue?logo=linkedin&style=flat-square\" height=\"25\">\n",
        "</a>\n",
        "<a href=\"https://github.com/arooshahmad-data\" target=\"_blank\">\n",
        "<img src=\"https://img.shields.io/badge/GitHub-181717?logo=github&style=flat-square\" height=\"25\">\n",
        "</a>\n",
        "<a href=\"https://www.kaggle.com/arooshahmadds\" target=\"_blank\">\n",
        "<img src=\"https://img.shields.io/badge/Kaggle-20BEFF?logo=kaggle&style=flat-square\" height=\"25\">\n",
        "</a>\n",
        "</center>\n",
        "\"\"\", unsafe_allow_html=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfCD-ErX2j4S",
        "outputId": "1957865b-c8b8-4ec4-b140-7c4ce65a7543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class CNNLSTMModel(nn.Module):\n",
        "    def __init__(self, hidden_dim=256, lstm_layers=1, num_classes=2):\n",
        "        super(CNNLSTMModel, self).__init__()\n",
        "\n",
        "        # Pretrained CNN (ResNet18)\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n",
        "        self.feature_dim = 512  # ResNet18 final feature size\n",
        "\n",
        "        # Freeze CNN (optional)\n",
        "        for param in self.cnn.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # LSTM\n",
        "        self.lstm = nn.LSTM(input_size=self.feature_dim, hidden_size=hidden_dim,\n",
        "                            num_layers=lstm_layers, batch_first=True)\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):  # x: [B, T, C, H, W]\n",
        "        B, T, C, H, W = x.shape\n",
        "        x = x.view(B * T, C, H, W)\n",
        "\n",
        "        features = self.cnn(x)  # Output: (B*T, 512, 1, 1)\n",
        "\n",
        "        features = features.view(B, T, self.feature_dim)  # Reshape to (B, T, 512)\n",
        "        lstm_out, _ = self.lstm(features)  # (B, T, hidden_dim)\n",
        "        final_output = lstm_out[:, -1, :]  # Take last timestep\n",
        "        out = self.classifier(final_output)  # (B, num_classes)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YttNQq4F3Mx6",
        "outputId": "28f3ca7f-e7b3-4d7f-a266-27a5c6362c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running app"
      ],
      "metadata": {
        "id": "jQMbGY4I3gbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change Working Directory\n",
        "%cd /content/drive/MyDrive/violence_detection"
      ],
      "metadata": {
        "id": "5SM0z659tOOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear Streamlit Cache\n",
        "import streamlit as st\n",
        "st.cache_data.clear()\n",
        "st.cache_resource.clear()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgJyjsgO18jr",
        "outputId": "7c333a06-6d24-46c7-f58a-fcbbbb212282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-06-17 08:07:16.719 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** kill the streamlit process and ngrok tunnel in case cache does not get's cleared."
      ],
      "metadata": {
        "id": "g4aI5_fl75es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kill all previous sessions\n",
        "!pkill streamlit\n",
        "!pkill -f streamlit\n",
        "!pkill -f ngrok\n",
        "\n",
        "!rm -rf ~/.streamlit/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ82DrVl4YEV",
        "outputId": "854dd17d-03a4-43ae-f0f2-b41ca3823bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-06-17T08:07:18+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-bbc96445-ad27-434b-8bfa-ab3c34f9a8a3 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill previous tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Wait to avoid race condition\n",
        "# time.sleep(3)\n",
        "\n",
        "# Start ngrok with correct syntax\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\")\n",
        "print(f\"ðŸ”— Streamlit App is Live: {public_url}\")\n",
        "\n",
        "# Run streamlit in background\n",
        "!streamlit run app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSz548sG3b1c",
        "outputId": "cb64dd82-4f83-4e9d-a7e2-501d88242871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”— Streamlit App is Live: NgrokTunnel: \"https://f999-34-148-30-96.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Misc Collect Video"
      ],
      "metadata": {
        "id": "xkBLBSBdG3wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from google.colab import files\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "\n",
        "def create_video_from_frames(video_id, folder_path, fps=25, violence = True):\n",
        "    \"\"\"\n",
        "      Combines frames of video back to video format (.mp4) at 25 fps.\n",
        "    \"\"\"\n",
        "    if violence:\n",
        "      pattern = os.path.join(folder_path, f\"V_{video_id}.mp4_frame*.jpg\")\n",
        "    else:\n",
        "      pattern = os.path.join(folder_path, f\"NV_{video_id}.mp4_frame*.jpg\")\n",
        "\n",
        "\n",
        "    frame_files = natsorted(glob(pattern))\n",
        "\n",
        "    if not frame_files:\n",
        "        print(f\"âŒ No frames found for video ID: {video_id}\")\n",
        "        return\n",
        "\n",
        "    # Read the first frame to get dimensions\n",
        "    first_frame = cv2.imread(frame_files[0])\n",
        "    height, width, layers = first_frame.shape\n",
        "\n",
        "    if violence:\n",
        "      output_name = f\"V_{video_id}.mp4\"\n",
        "    else:\n",
        "      output_name = f\"NV_{video_id}.mp4\"\n",
        "\n",
        "\n",
        "    out = cv2.VideoWriter(output_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "\n",
        "    print(f\"ðŸ”„ Generating video for ID {video_id} with {len(frame_files)} frames...\")\n",
        "\n",
        "    for frame_file in frame_files:\n",
        "        frame = cv2.imread(frame_file)\n",
        "        out.write(frame)\n",
        "\n",
        "    out.release()\n",
        "    print(f\"âœ… Video created: {output_name}\")\n",
        "\n",
        "    files.download(output_name)\n",
        "\n",
        "# ---------- ðŸ”§ User Configuration ----------\n",
        "\n",
        "# Path to folder containing all frame images\n",
        "folder_path = \"/content/drive/MyDrive/violence_detection/images\"  # Update if frames are elsewhere\n",
        "\n",
        "# List of video IDs to process (just numbers, not filenames)\n",
        "video_ids = [1000, 223, 512, 645, 798, 919]  # Add as many IDs as needed\n",
        "\n",
        "# ---------- ðŸŽ¬ Run Conversion ----------\n",
        "for vid in video_ids:\n",
        "    create_video_from_frames(vid, folder_path, violence=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "8zhBsnxV3qUm",
        "outputId": "465025b5-91eb-4a1c-8105-362fe70e6272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Generating video for ID 1000 with 5 frames...\n",
            "âœ… Video created: NV_1000.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_21a32403-d9ec-44cc-a1ff-fd088f751e65\", \"NV_1000.mp4\", 107600)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Generating video for ID 223 with 5 frames...\n",
            "âœ… Video created: NV_223.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_49962fc3-852c-48ba-bea6-a727e057d7a5\", \"NV_223.mp4\", 41329)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Generating video for ID 512 with 5 frames...\n",
            "âœ… Video created: NV_512.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c1d09661-77f5-49e9-a8d3-8bd6c5dcf1c4\", \"NV_512.mp4\", 32849)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Generating video for ID 645 with 5 frames...\n",
            "âœ… Video created: NV_645.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_32507270-43ff-4dbd-8b5c-2eeee07a3f09\", \"NV_645.mp4\", 21401)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Generating video for ID 798 with 5 frames...\n",
            "âœ… Video created: NV_798.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4d520135-1dc4-41c7-ba94-866cb16dc3e2\", \"NV_798.mp4\", 87660)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Generating video for ID 919 with 5 frames...\n",
            "âœ… Video created: NV_919.mp4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_24aecd08-1938-43f1-a2e9-87b18a649d1a\", \"NV_919.mp4\", 47363)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Combining small chunks from videos\n",
        "\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "import os\n",
        "import shutil\n",
        "from IPython.display import Javascript\n",
        "\n",
        "# --- User Inputs ---\n",
        "folder_path = \"/content/drive/MyDrive/violence_detection/videos\"  # âœ… CHANGE this to your Drive folder path\n",
        "prefix = \"NV_\"\n",
        "output_file = \"NV_combined_video.mp4\"\n",
        "save_to_drive = True  # Set to False to trigger browser download instead\n",
        "\n",
        "# --- Collect matching video files ---\n",
        "video_files = sorted([\n",
        "    os.path.join(folder_path, f)\n",
        "    for f in os.listdir(folder_path)\n",
        "    if f.startswith(prefix) and f.endswith((\".mp4\", \".mov\", \".avi\"))\n",
        "])\n",
        "\n",
        "if not video_files:\n",
        "    print(\"âŒ No matching videos found.\")\n",
        "else:\n",
        "    print(f\"ðŸŽ¬ Found {len(video_files)} video(s). Combining...\")\n",
        "\n",
        "    # --- Load and concatenate ---\n",
        "    clips = [VideoFileClip(f) for f in video_files]\n",
        "    final_clip = concatenate_videoclips(clips)\n",
        "\n",
        "    # --- Save temp output ---\n",
        "    temp_path = f\"/content/{output_file}\"\n",
        "    final_clip.write_videofile(temp_path, codec=\"libx264\", audio_codec=\"aac\")\n",
        "\n",
        "    if save_to_drive:\n",
        "        # --- Save to same Drive folder ---\n",
        "        dest_path = os.path.join(folder_path, output_file)\n",
        "        shutil.move(temp_path, dest_path)\n",
        "        print(f\"âœ… Saved to Google Drive: {dest_path}\")\n",
        "    else:\n",
        "        # --- Trigger browser download ---\n",
        "        print(\"â¬‡ï¸ Downloading to browser...\")\n",
        "        display(Javascript(f'''\n",
        "            var link = document.createElement('a');\n",
        "            link.href = \"{temp_path}\";\n",
        "            link.download = \"{output_file}\";\n",
        "            document.body.appendChild(link);\n",
        "            link.click();\n",
        "            document.body.removeChild(link);\n",
        "        '''))\n"
      ],
      "metadata": {
        "id": "zDJTF23Sy59T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Combined Video\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Configuration ---\n",
        "folder_path = \"/content/drive/MyDrive/MSAI/real-life-violence-and-nonviolence-data/frames\"  # Your frames folder\n",
        "prefix = \"NV_\"  # Video ID/prefix like \"NV_\"\n",
        "output_path = \"/content/NV_combined_framewise.mp4\"\n",
        "fps = 5  # You can change this\n",
        "\n",
        "# --- Step 1: Group frames by video ---\n",
        "frame_dict = {}\n",
        "\n",
        "for path in glob(os.path.join(folder_path, f\"{prefix}*.jpg\")):\n",
        "    basename = os.path.basename(path)\n",
        "    parts = basename.replace(\".jpg\", \"\").split(\"_\")  # Example: NV_1_03 â†’ ['NV', '1', '03']\n",
        "\n",
        "    if len(parts) < 3:\n",
        "        continue  # Skip invalid names\n",
        "\n",
        "    video_id = f\"{parts[0]}_{parts[1]}\"  # e.g., NV_1\n",
        "    frame_idx = parts[2]\n",
        "\n",
        "    if frame_idx not in frame_dict:\n",
        "        frame_dict[frame_idx] = []\n",
        "\n",
        "    frame_dict[frame_idx].append((video_id, path))\n",
        "\n",
        "# --- Step 2: Sort frame indexes ---\n",
        "frame_keys = natsorted(frame_dict.keys())\n",
        "\n",
        "# --- Step 3: Stack frames and write to video ---\n",
        "video_writer = None\n",
        "frame_size = None\n",
        "\n",
        "for frame_idx in tqdm(frame_keys, desc=\"Processing frames\"):\n",
        "    frame_paths = natsorted([p[1] for p in frame_dict[frame_idx]])\n",
        "\n",
        "    loaded_frames = [cv2.imread(fp) for fp in frame_paths]\n",
        "    loaded_frames = [cv2.resize(f, (224, 224)) for f in loaded_frames if f is not None]\n",
        "\n",
        "    if not loaded_frames:\n",
        "        continue\n",
        "\n",
        "    # Combine horizontally\n",
        "    combined_frame = np.hstack(loaded_frames)\n",
        "\n",
        "    # Initialize video writer\n",
        "    if video_writer is None:\n",
        "        height, width = combined_frame.shape[:2]\n",
        "        frame_size = (width, height)\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "        video_writer = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
        "\n",
        "    video_writer.write(combined_frame)\n",
        "\n",
        "# --- Release Writer ---\n",
        "if video_writer:\n",
        "    video_writer.release()\n",
        "    print(f\"âœ… Combined video saved at: {output_path}\")\n",
        "else:\n",
        "    print(\"âŒ No frames found or video not created.\")\n"
      ],
      "metadata": {
        "id": "cpMW-0LoAgHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance Summary\n",
        "\n",
        "| Model                | Spatial Features | Temporal Modeling | Train Acc | Val Acc | Test Acc | Remarks                                         |\n",
        "|---------------------|------------------|-------------------|-----------|---------|----------|--------------------------------------------------|\n",
        "| **CNNVideoClassifier** | âœ… ResNet18       | âŒ None             | ~82.0%     | ~89.0%   |  ~78      | Fast & light, lacks temporal context             |\n",
        "| **CNN + LSTM**         | âœ… ResNet18       | âœ… LSTM             | ~95%      | ~94%    | ~92%     | Best for temporal data (e.g. violence detection) |\n",
        "| **3D CNN**             | âœ… 3D ConvNet     | âœ… Implicit         | ~87%      | ~85%    | -     | Captures spatiotemporal patterns directly        |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wziDhbGZPYta"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "yjRxLSOt3pH6",
        "M1LlSKfPcGdM",
        "gz0-XUkGwF6m",
        "YoOFDaLYwIEp",
        "1dmjphlr1oqG",
        "2zZdzIVhpBW2"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}